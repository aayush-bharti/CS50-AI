
During my experimentation project, I tested several factors of the neural network. I first started off by tested out the amount of hidden layers inside the neural networks. I started with 1 and then increased them by 1 each time trying to find the amount that would give me the most accuracy and least loss. I didn't want to put too many hidden layers as well as this could make it less accurate. After this, I tested the amount of Convolutional layers and Pooling layers. I tried it out with just 1 of each, but it did not seem like it was accurate enough, so I added one more. I tried out just adding 1 Convolutional layer and keeping the same number of Pooling layers, but adding another Pooling layer worked better. I also tested out the kernel sizes with (4,4) on one of them first and then both of them after, but it made the results worse. I tried this with the Pooling layer sizes as well by using (3,3) instead of (2,2) but this did not work either. The last thing I tested out was the dropout value. I tested out values from 0.25 to 0.5 to 0.8, but the best results seem to have came from the 0.5 and 0.6 value. I continued to test values in between and also just those 2 values, and I found out that 0.5 would work best. 